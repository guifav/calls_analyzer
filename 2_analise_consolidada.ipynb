{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j5i446BGhae9",
        "6ZrDdGIPOs8h",
        "-uCHDICTPxmB",
        "p6GzdO_gQNmV"
      ],
      "mount_file_id": "1bAfRGhxJuwqiORpMzPjhKWEpt1Ax-VNs",
      "authorship_tag": "ABX9TyOLzYSz/B2HsCD+Cl2+8qWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guifav/calls_analyzer/blob/main/2_analise_consolidada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando recursos necessários"
      ],
      "metadata": {
        "id": "j5i446BGhae9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando biblioteca Openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "zJxWaCmbP7D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar secret keys\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Configurar o cliente OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "yz0MQjl7gsc3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise gráfica dos resultados"
      ],
      "metadata": {
        "id": "6ZrDdGIPOs8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSgtWubcOn9P"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download dos recursos necessários do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Função para carregar os dados JSON\n",
        "def load_json_data(sdr_name):\n",
        "    sdr_name = sdr_name.replace('_transcriptions.json', '')\n",
        "    file_path = f'/content/drive/MyDrive/ai_gri/{sdr_name}/{sdr_name}_transcriptions.json'\n",
        "    print(f\"Tentando abrir: {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        print(f\"Leitura do arquivo: {file_path}\")\n",
        "        return json.load(file)\n",
        "\n",
        "# Função para análise de sentimento\n",
        "def analyze_sentiment(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    return sia.polarity_scores(text)['compound']\n",
        "\n",
        "# Função para extrair tópicos\n",
        "def extract_topics(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Função para calcular a duração da chamada\n",
        "def calculate_call_duration(timestamp):\n",
        "    call_time = datetime.strptime(timestamp.split('_')[0], \"%Y-%m-%dT%H-%M-%S\")\n",
        "    return (datetime.now() - call_time).total_seconds() / 60  # duração em minutos\n",
        "\n",
        "# Função para verificar o sucesso da chamada\n",
        "def check_success(text):\n",
        "    success_keywords = ['schedule', 'interested', 'follow up', 'next steps']\n",
        "    return any(keyword in text.lower() for keyword in success_keywords)\n",
        "\n",
        "# Função para identificar objeções\n",
        "def identify_objections(text):\n",
        "    objection_keywords = ['expensive', 'not interested', 'busy', 'call back', 'no budget']\n",
        "    return [keyword for keyword in objection_keywords if keyword in text.lower()]\n",
        "\n",
        "# Função para extrair palavras-chave\n",
        "def extract_keywords(text, n=10):\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return Counter(keywords).most_common(n)\n",
        "\n",
        "# Função para criar nuvem de palavras\n",
        "def create_word_cloud(data):\n",
        "    all_text = ' '.join([transcript for sdr_data in data.values() for transcript in sdr_data.values()])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nuvem de Palavras de Todas as Transcrições')\n",
        "    plt.show()\n",
        "\n",
        "# Funções de análise\n",
        "\n",
        "def analyze_sentiment_distribution(data):\n",
        "    sentiments = [analyze_sentiment(transcript) for sdr_data in data.values() for transcript in sdr_data.values()]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sentiments, bins=20, edgecolor='black')\n",
        "    plt.title('Distribuição de Sentimentos')\n",
        "    plt.xlabel('Pontuação de Sentimento')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "def analyze_call_durations(data):\n",
        "    durations = [calculate_call_duration(call_id) for sdr_data in data.values() for call_id in sdr_data.keys()]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(durations, bins=20, edgecolor='black')\n",
        "    plt.title('Distribuição de Duração das Chamadas')\n",
        "    plt.xlabel('Duração (minutos)')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "def analyze_topics(data):\n",
        "    all_topics = [topic for sdr_data in data.values() for transcript in sdr_data.values() for topic in extract_topics(transcript)]\n",
        "    topic_counts = Counter(all_topics).most_common(10)\n",
        "    topics, counts = zip(*topic_counts)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(topics, counts)\n",
        "    plt.title('Top 10 Tópicos Mais Comuns')\n",
        "    plt.xlabel('Tópicos')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_success_rate(data):\n",
        "    success_rates = {sdr: sum(check_success(transcript) for transcript in transcripts.values()) / len(transcripts)\n",
        "                     for sdr, transcripts in data.items()}\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(success_rates.keys(), success_rates.values())\n",
        "    plt.title('Taxa de Sucesso por SDR')\n",
        "    plt.xlabel('SDR')\n",
        "    plt.ylabel('Taxa de Sucesso')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "def analyze_common_objections(data):\n",
        "    all_objections = [objection for sdr_data in data.values() for transcript in sdr_data.values() for objection in identify_objections(transcript)]\n",
        "    objection_counts = Counter(all_objections).most_common(5)\n",
        "    objections, counts = zip(*objection_counts)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(objections, counts)\n",
        "    plt.title('Top 5 Objeções Mais Comuns')\n",
        "    plt.xlabel('Objeções')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Função principal de análise\n",
        "def analyze_transcriptions(data):\n",
        "    analyze_sentiment_distribution(data)\n",
        "    analyze_call_durations(data)\n",
        "    analyze_topics(data)\n",
        "    analyze_success_rate(data)\n",
        "    analyze_common_objections(data)\n",
        "    create_word_cloud(data)\n",
        "\n",
        "    results = {\n",
        "        'sentiment_scores': [analyze_sentiment(transcript) for sdr_data in data.values() for transcript in sdr_data.values()],\n",
        "        'call_durations': [calculate_call_duration(call_id) for sdr_data in data.values() for call_id in sdr_data.keys()],\n",
        "        'topics': Counter([topic for sdr_data in data.values() for transcript in sdr_data.values() for topic in extract_topics(transcript)]),\n",
        "        'success_rate': {sdr: sum(check_success(transcript) for transcript in transcripts.values()) / len(transcripts) for sdr, transcripts in data.items()},\n",
        "        'common_objections': Counter([objection for sdr_data in data.values() for transcript in sdr_data.values() for objection in identify_objections(transcript)]),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Carregamento dos dados\n",
        "jauzibeth_data = load_json_data('jauzibeth_nieves')\n",
        "lucas_data = load_json_data('lucas_belelli')\n",
        "marcelo_data = load_json_data('marcelo_machado')\n",
        "\n",
        "all_data = {\n",
        "    'Jauzibeth': jauzibeth_data,\n",
        "    'Lucas': lucas_data,\n",
        "    'Marcelo': marcelo_data\n",
        "}\n",
        "\n",
        "# Execução da análise\n",
        "results = analyze_transcriptions(all_data)\n",
        "\n",
        "# Impressão dos resultados numéricos\n",
        "print(\"Resultados numéricos:\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}:\")\n",
        "    if isinstance(value, dict):\n",
        "        for sub_key, sub_value in value.items():\n",
        "            print(f\"  {sub_key}: {sub_value}\")\n",
        "    elif isinstance(value, Counter):\n",
        "        for item, count in value.most_common(10):\n",
        "            print(f\"  {item}: {count}\")\n",
        "    else:\n",
        "        print(f\"  {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análises individuais"
      ],
      "metadata": {
        "id": "-uCHDICTPxmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up OpenAI client\n",
        "client = OpenAI(api_key='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')\n",
        "\n",
        "# Function to load JSON data\n",
        "def load_json_data(sdr_name):\n",
        "    sdr_name = sdr_name.replace('_transcriptions.json', '')\n",
        "    file_path = f'/content/drive/MyDrive/ai_gri/{sdr_name}/{sdr_name}_transcriptions.json'\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Function to analyze JSON file using OpenAI\n",
        "def analyze_json(json_data):\n",
        "    # Convert JSON to a formatted string\n",
        "    json_str = json.dumps(json_data, indent=2)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analise o seguinte arquivo JSON contendo transcrições de ligações de representantes de desenvolvimento de vendas (SDRs)\n",
        "\n",
        "    {json_str}\n",
        "\n",
        "    Por favor, forneça insights sobre:\n",
        "    Nuvem de palavras-chave relacionadas à venda: Identifique e liste as palavras-chave mais frequentes e relevantes que aparecem nas transcrições e que estão relacionadas ao processo de venda.\n",
        "    Principais objeções: Destaque as objeções mais comuns levantadas pelos clientes durante as ligações, fornecendo exemplos específicos quando possível.\n",
        "    Padrões positivos nas ligações: Identifique padrões ou práticas positivas que contribuíram para o sucesso das ligações, como técnicas de persuasão eficazes, frases que geraram interesse ou respostas positivas dos clientes.\n",
        "    Padrões negativos nas ligações: Detecte padrões ou práticas negativas que possam estar prejudicando o desempenho das ligações, como respostas inadequadas, falhas de comunicação ou falta de preparação.\n",
        "    Treinamentos sugeridos para desenvolvimento do SDR: Com base nos padrões observados, sugira treinamentos ou áreas de desenvolvimento que podem ajudar os SDRs a melhorar seu desempenho, fornecendo exemplos práticos de como esses treinamentos poderiam ser aplicados.\n",
        "\n",
        "    Seja específico e forneça exemplos do JSON sempre que possível.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert data analyst specializing in sales performance data.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Load and analyze data for each SDR\n",
        "sdrs = ['jauzibeth_nieves', 'lucas_belelli', 'marcelo_machado']\n",
        "\n",
        "for sdr in sdrs:\n",
        "    print(f\"\\nAnalyzing JSON file for {sdr}:\")\n",
        "    json_data = load_json_data(sdr)\n",
        "    analysis = analyze_json(json_data)\n",
        "    print(analysis)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Note: This script will make API calls to OpenAI for each JSON file,\n",
        "# which may take some time and consume API credits."
      ],
      "metadata": {
        "id": "L7IPfGAUPznM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análises combinadas"
      ],
      "metadata": {
        "id": "p6GzdO_gQNmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up OpenAI client\n",
        "client = OpenAI(api_key='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')\n",
        "\n",
        "# Function to load JSON data\n",
        "def load_json_data(sdr_name):\n",
        "    sdr_name = sdr_name.replace('_transcriptions.json', '')\n",
        "    file_path = f'/content/drive/MyDrive/ai_gri/{sdr_name}/{sdr_name}_transcriptions.json'\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Function to analyze JSON file using OpenAI\n",
        "def analyze_json(json_data):\n",
        "    # Convert JSON to a formatted string\n",
        "    json_str = json.dumps(json_data, indent=2)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analise o seguinte arquivo JSON contendo transcrições de ligações de representantes de desenvolvimento de vendas (SDRs)\n",
        "\n",
        "    {json_str}\n",
        "\n",
        "    Por favor, forneça insights sobre:\n",
        "    Nuvem de palavras-chave relacionadas à venda: Identifique e liste as palavras-chave mais frequentes e relevantes que aparecem nas transcrições e que estão relacionadas ao processo de venda.\n",
        "    Principais objeções: Destaque as objeções mais comuns levantadas pelos clientes durante as ligações, fornecendo exemplos específicos quando possível.\n",
        "    Padrões positivos nas ligações: Identifique padrões ou práticas positivas que contribuíram para o sucesso das ligações, como técnicas de persuasão eficazes, frases que geraram interesse ou respostas positivas dos clientes.\n",
        "    Padrões negativos nas ligações: Detecte padrões ou práticas negativas que possam estar prejudicando o desempenho das ligações, como respostas inadequadas, falhas de comunicação ou falta de preparação.\n",
        "    Treinamentos sugeridos para desenvolvimento do SDR: Com base nos padrões observados, sugira treinamentos ou áreas de desenvolvimento que podem ajudar os SDRs a melhorar seu desempenho, fornecendo exemplos práticos de como esses treinamentos poderiam ser aplicados.\n",
        "\n",
        "    Seja específico e forneça exemplos do JSON sempre que possível.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert data analyst specializing in sales performance data.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function to save consolidated results to json file\n",
        "def save_consolidated_json(data, file_name):\n",
        "    # Definir o caminho completo do arquivo no Google Drive\n",
        "    file_path = os.path.join('/content/drive/MyDrive/ai_gri', file_name)\n",
        "\n",
        "    try:\n",
        "        # Criar diretórios se não existirem\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Salvar o conteúdo no arquivo JSON\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(data, file, ensure_ascii=False, indent=2)\n",
        "        print(f\"JSON consolidado salvo com sucesso em: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao salvar o arquivo JSON: {str(e)}\")\n",
        "\n",
        "# Function to save analysis result to a markdown file\n",
        "def save_to_markdown(content, file_name):\n",
        "    # Definir o caminho completo do arquivo no Google Drive\n",
        "    file_path = os.path.join('/content/drive/MyDrive/ai_gri', file_name)\n",
        "\n",
        "    try:\n",
        "        # Criar diretórios se não existirem\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Salvar o conteúdo no arquivo\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(content)\n",
        "        print(f\"Análise salva com sucesso em: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao salvar o arquivo Markdown: {str(e)}\")\n",
        "\n",
        "# Load and combine data for all SDRs\n",
        "sdrs = ['jauzibeth_nieves', 'lucas_belelli', 'marcelo_machado']\n",
        "combined_data = []\n",
        "\n",
        "for sdr in sdrs:\n",
        "    print(f\"Carregando arquivo JSON para {sdr}...\")\n",
        "    json_data = load_json_data(sdr)\n",
        "    combined_data.append(json_data)\n",
        "\n",
        "combined_json_data = {\"sdrs\": combined_data}\n",
        "\n",
        "# Salvar o JSON consolidado\n",
        "save_consolidated_json(combined_json_data, 'consolidated_data.json')\n",
        "\n",
        "# Analisar dados JSON combinados\n",
        "analysis = analyze_json(combined_json_data)\n",
        "print(\"Resultado da Análise:\")\n",
        "print(analysis)\n",
        "\n",
        "# Salvar o resultado da análise em um arquivo markdown\n",
        "save_to_markdown(analysis, 'analysis_result.md')"
      ],
      "metadata": {
        "id": "pLyNSFAlQPmF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}